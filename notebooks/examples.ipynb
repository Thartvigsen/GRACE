{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda75d0c",
   "metadata": {},
   "source": [
    "# Editing a T5 QA model with GRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc82f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grace\n",
    "from grace.editors import GRACE_barebones as GRACE\n",
    "from grace.utils import tokenize_qa\n",
    "import torch\n",
    "import copy\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89338b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-small-ssm-nq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-small-ssm-nq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de96ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_edit = \"encoder.block[4].layer[1].DenseReluDense.wo\" # Which layer to edit?\n",
    "init_epsilon = 3.0 # Initial epsilon for GRACE codebook entries\n",
    "learning_rate = 1.0 # Learning rate with which to learn new GRACE values\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "original_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1e66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- wrap model with GRACE ---\n",
    "edited_model = GRACE(model, layer_to_edit, init_epsilon, learning_rate, device, generation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e734240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Editing. Question: How tall is the empire state building?. Answer: 71 ft\n"
     ]
    }
   ],
   "source": [
    "# --- Desired edit ---\n",
    "edit_input = {\n",
    "    \"text\": [\"How tall is the empire state building?\"],\n",
    "    \"labels\": [\"1,454 feet\"],\n",
    "}\n",
    "\n",
    "edit_tokens = tokenize_qa(edit_input, tokenizer, device)\n",
    "\n",
    "# --- Check model's prediction for this edit before applying the edit ---\n",
    "preds = original_model.generate(edit_tokens[\"input_ids\"]).squeeze()\n",
    "original_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"Before Editing. Question: {edit_input['text'][0]}. Answer: {original_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0d3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Apply the edit ---\n",
    "edited_model.edit(edit_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f86fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Editing. Question: How tall is the empire state building?. Answer: 1,454 feet\n"
     ]
    }
   ],
   "source": [
    "# --- Check model's prediction for this edit AFTER applying the edit ---\n",
    "preds = edited_model.generate(edit_tokens[\"input_ids\"]).squeeze()\n",
    "new_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"After Editing. Question: {edit_input['text'][0]}. Answer: {new_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c808c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Editing. Question: how high is the empire state building?. Answer: 57 ft\n"
     ]
    }
   ],
   "source": [
    "# --- Trying slightly different input text ---\n",
    "test_input = {\n",
    "    \"text\": [\"how high is the empire state building?\"],\n",
    "    \"labels\": [\"1,454 feet\"]\n",
    "}\n",
    "\n",
    "test_tokens = tokenize_qa(test_input, tokenizer, device)\n",
    "\n",
    "preds = edited_model.generate(test_tokens[\"input_ids\"], max_length=20).squeeze()\n",
    "new_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"After Editing. Question: {test_input['text'][0]}. Answer: {new_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e9e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Editing. Question: How tall is the eiffel tower?. Answer: 157 ft\n",
      "After Editing. Question: How tall is the eiffel tower?. Answer: 157 ft\n"
     ]
    }
   ],
   "source": [
    "# --- Check if the original and edited model have the same prediction on an unrelated input ---\n",
    "unrelated_input = {\n",
    "    \"text\": [\"How tall is the eiffel tower?\"],\n",
    "    \"labels\": [\"1,083 feet\"]\n",
    "}\n",
    "\n",
    "unrelated_tokens = tokenize_qa(unrelated_input, tokenizer, device)\n",
    "\n",
    "preds = original_model.generate(unrelated_tokens[\"input_ids\"]).squeeze()\n",
    "new_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"Before Editing. Question: {unrelated_input['text'][0]}. Answer: {new_answer}\")\n",
    "\n",
    "preds = edited_model.generate(unrelated_tokens[\"input_ids\"]).squeeze()\n",
    "new_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"After Editing. Question: {unrelated_input['text'][0]}. Answer: {new_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
